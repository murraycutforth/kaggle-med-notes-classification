{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-ownership",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import nltk\n",
    "import sklearn\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcoded variables\n",
    "\n",
    "DATA_DIR = Path('dataset/train-data')\n",
    "assert DATA_DIR.exists()\n",
    "\n",
    "VAL_SPLIT = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-logistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First off we just locate all the notes files and organise them by speciality\n",
    "\n",
    "speciality_dirs = list(DATA_DIR.glob('./*'))\n",
    "speciality_names = [x.name for x in speciality_dirs]\n",
    "speciality_to_label = {x: i for i, x in enumerate(speciality_names)}\n",
    "label_to_speciality = {v: k for k, v in speciality_to_label.items()}\n",
    "\n",
    "# Compile list of [path, label] pairs\n",
    "dataset = []\n",
    "for sp_name, sp_dir in zip(speciality_names, speciality_dirs):\n",
    "    for filepath in sp_dir.glob('*.txt'):\n",
    "        dataset.append([filepath, speciality_to_label[sp_name]])\n",
    "\n",
    "pprint(dataset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split off a training set and a validation set\n",
    "\n",
    "filepaths, labels = list(zip(*dataset))\n",
    "\n",
    "filepaths_train, filepaths_val, labels_train, labels_val = \\\n",
    "    sklearn.model_selection.train_test_split(filepaths, labels, test_size=VAL_SPLIT, random_state=42, stratify=labels)\n",
    "\n",
    "assert len(filepaths_train) == len(labels_train)\n",
    "print(len(filepaths_train))\n",
    "pprint(filepaths_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text preprocessing function here\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    # Remove html tags\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # Remove non-word chars\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace all whitespace with single space\n",
    "    text = re.sub(r'[^\\w ]', '', text) # Remove all non-word characters\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve raw text data\n",
    "\n",
    "def create_df(filepaths, labels):\n",
    "    text = []\n",
    "    for fpath in filepaths:\n",
    "        with open(fpath, 'r', encoding=\"utf8\", errors='ignore') as f:  # Needed to cope with non-ascii characters\n",
    "            text.append(f.read())\n",
    "\n",
    "    return pd.DataFrame({'filepath': filepaths, 'label': labels, 'raw_text': text, 'processed_text': [preprocess(x) for x in text]})\n",
    "\n",
    "train_df = create_df(filepaths_train, labels_train)\n",
    "val_df = create_df(filepaths_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-capture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BOW feature vector for each doc\n",
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(stop_words='english')\n",
    "X_train = count_vectorizer.fit_transform(train_df['processed_text'])\n",
    "X_val = count_vectorizer.transform(val_df['processed_text'])\n",
    "\n",
    "# Train linear SVM\n",
    "model = sklearn.svm.LinearSVC(max_iter=1000, C=0.001)\n",
    "model.fit(X_train, train_df['label'])\n",
    "\n",
    "# Eval predictions on val set\n",
    "y_pred = model.predict(X_val)\n",
    "acc = sklearn.metrics.accuracy_score(val_df['label'], y_pred)\n",
    "\n",
    "print('Accuracy = {:f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-disease",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try again, but with TF-IDF features\n",
    "\n",
    "count_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(stop_words='english')\n",
    "X_train = count_vectorizer.fit_transform(train_df['processed_text'])\n",
    "X_val = count_vectorizer.transform(val_df['processed_text'])\n",
    "\n",
    "model = sklearn.svm.LinearSVC(max_iter=1000, C=1)\n",
    "model.fit(X_train, train_df['label'])\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "acc = sklearn.metrics.accuracy_score(val_df['label'], y_pred)\n",
    "\n",
    "print('Accuracy = {:f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sklearn.metrics.plot_confusion_matrix(model, X_val, val_df['label'], normalize='true', display_labels=speciality_names, xticks_rotation='vertical')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
